{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA5iwOrtAmbo"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # for array\n",
        "import pandas as pd  # for csv files and dataframe\n",
        "import matplotlib.pyplot as plt  # for plotting\n",
        "import seaborn as sns  # plotting\n",
        "from scipy import stats\n",
        "\n",
        "import pickle  # To load data int disk\n",
        "from prettytable import PrettyTable  # To print in tabular format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
        "from sklearn.metrics import auc, f1_score, roc_curve\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqeJ8omQiGHg"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_training-set.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_testing-set.csv')\n",
        "df = pd.concat([train, test]).drop(['id'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7HDql4djqN8",
        "outputId": "66502636-9d23-4020-de52-b06b42fcee93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
            "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
            "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
            "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
            "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
            "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
            "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
            "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9utcQNe1jxqX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Perform necessary preprocessing\n",
        "# Drop unnecessary columns\n",
        "df.drop(['dur', 'proto', 'service', 'state'], axis=1, inplace=True)\n",
        "# Replace missing values with 0\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Convert categorical features to numeric using LabelEncoder\n",
        "le = LabelEncoder()\n",
        "categorical_features = ['spkts', 'dpkts', 'sbytes',\n",
        "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
        "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
        "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
        "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
        "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
        "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
        "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label']\n",
        "# Apply RobustScaler to normalize numeric features\n",
        "numeric_features = df.select_dtypes(include=['float64']).columns\n",
        "scaler = RobustScaler()\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "  \n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "for col in categorical_features:\n",
        "  df[col]=df[col].astype(str)\n",
        "  df[col]=label_encoder.fit_transform(df[col])\n",
        "for feature in categorical_features:\n",
        "    df[feature] = le.fit_transform(df[feature])\n",
        "\n",
        "\n",
        "\n",
        "# Further analysis with the selected features\n",
        "# X_train and X_test contain the selected features based on ANOVA and Information Gain\n",
        "# y_train and y_test are the corresponding target labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def is_float(element: any) -> bool:\n",
        "    #If you expect None to be passed:\n",
        "    if element is None: \n",
        "        return False\n",
        "    try:\n",
        "        float(element)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "df=df.replace('?',np.nan)\n",
        "df=df.replace('-',np.nan)\n",
        "\n",
        "for col in df.columns:\n",
        "  df=df[df[col].apply(lambda x: is_float(str(x)))]\n",
        "  if df[col].dtype == 'object':\n",
        "    df[col]=df[col].astype(float)\n",
        "  df[col]=df[col].fillna(df[col].mean())"
      ],
      "metadata": {
        "id": "lVA2sD--Tjo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWsc3sFZ-erJ"
      },
      "outputs": [],
      "source": [
        "# Perform feature selection using ANOVA and Information Gain\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = df.drop('label', axis=1)\n",
        "y = df['label']\n",
        " # Number of top features to select\n",
        "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
        "X_enn, y_enn = enn.fit_resample(X, y)\n",
        "k = int(len(X_enn.columns) * 0.33) \n",
        "\n",
        "# Correlation Coefficient\n",
        "corr_top_k_features = []\n",
        "for feature in X.columns:\n",
        "    corr, _ = pearsonr(X[feature], y)\n",
        "    if abs(corr) >= 0.1:  # Set correlation coefficient threshold\n",
        "        corr_top_k_features.append(feature)\n",
        "\n",
        "\n",
        "# ANOVA\n",
        "f_selector = SelectKBest(f_classif, k=k)\n",
        "f_selector.fit(X, y)\n",
        "f_feature_scores = f_selector.scores_\n",
        "f_top_k_idx = f_feature_scores.argsort()[-k:][::-1]\n",
        "f_top_k_features = X.columns[f_top_k_idx].tolist()\n",
        "\n",
        "# Information Gain\n",
        "mi_selector = SelectKBest(mutual_info_classif, k=k)\n",
        "mi_selector.fit(X, y)\n",
        "mi_feature_scores = mi_selector.scores_\n",
        "mi_top_k_idx = mi_feature_scores.argsort()[-k:][::-1]\n",
        "mi_top_k_features = X.columns[mi_top_k_idx].tolist()\n",
        "\n",
        "\n",
        "# Select common top features from ANOVA, Information Gain, and Correlation Coefficient\n",
        "top_k_features = list(set(f_top_k_features).intersection(mi_top_k_features).intersection(corr_top_k_features))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the Isolation Forest model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "X=df[top_k_features]\n",
        "clf = IsolationForest(n_estimators=100, random_state=42, contamination='auto')\n",
        "clf.fit(X)\n",
        "\n",
        "# Predict the anomaly scores for each data point\n",
        "scores = clf.decision_function(X)\n",
        "\n",
        "# Find the indices of the outliers\n",
        "outlier_indices = np.where(clf.predict(X) == -1)[0]\n",
        "\n",
        "# Remove the outliers from the dataset\n",
        "data_clean = df.drop(df.index[outlier_indices])\n",
        "\n",
        "# Split the cleaned data into features (X_clean) and labels (y_clean)\n",
        "X_clean = data_clean.drop(['label'], axis=1)\n",
        "y_clean = data_clean['label']\n"
      ],
      "metadata": {
        "id": "9C-LToxfgKAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "vJ-KpGFFnfVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDwH816fPxH-",
        "outputId": "df18a8ac-fcc8-4394-b7a3-627ecf584146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, LSTM, Dense, Flatten\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Reshape X_clean to 3D array (samples, timesteps, features)\n",
        "X_clean = np.array(X_clean)\n",
        "X_clean = X_clean.reshape(X_clean.shape[0], X_clean.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the CNN LSTM model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_clean.shape[1], 1)))\n",
        "model.add(LSTM(units=64, activation='relu'))\n",
        "model.add(Dense(units=1, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "metadata": {
        "id": "Nm0NXSlZjr76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bff3d2-287e-4027-fdad-c481f6b012bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2979/2979 [==============================] - 84s 28ms/step - loss: nan - accuracy: 0.2038\n",
            "Epoch 2/10\n",
            "2979/2979 [==============================] - 77s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 3/10\n",
            "2979/2979 [==============================] - 79s 27ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 4/10\n",
            "2979/2979 [==============================] - 80s 27ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 5/10\n",
            "2979/2979 [==============================] - 78s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 6/10\n",
            "2979/2979 [==============================] - 77s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 7/10\n",
            "2979/2979 [==============================] - 78s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 8/10\n",
            "2979/2979 [==============================] - 79s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 9/10\n",
            "2979/2979 [==============================] - 78s 26ms/step - loss: nan - accuracy: 0.1480\n",
            "Epoch 10/10\n",
            "2979/2979 [==============================] - 81s 27ms/step - loss: nan - accuracy: 0.1480\n",
            "745/745 [==============================] - 8s 10ms/step - loss: nan - accuracy: 0.1464\n",
            "Test loss: nan\n",
            "Test accuracy: 0.14643052220344543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ = X_train.reshape((X_train.shape[0],1,X_train.shape[1],1))\n",
        "X_test_ = X_test.reshape((X_test.shape[0],1,X_test.shape[1],1))"
      ],
      "metadata": {
        "id": "tWfGH8rJb_-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvwG2IkzEfYY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.preprocessing import MinMaxScaler as SC\n",
        "\n",
        "\n",
        "input_ = keras.layers.Input(shape=(None,39,1))\n",
        "cnn1 = keras.layers.TimeDistributed(keras.layers.Conv1D(filters = 128,kernel_size =3,activation = 'relu'),input_shape=(None,39,1))(input_)\n",
        "Norm1 = keras.layers.TimeDistributed(keras.layers.BatchNormalization())(cnn1)\n",
        "Pool1 = keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=2, strides=2))(Norm1)\n",
        "cnn2 = keras.layers.TimeDistributed(keras.layers.Conv1D(filters=64, kernel_size=3,activation='relu'))(Pool1)\n",
        "Norm2 = keras.layers.TimeDistributed(keras.layers.BatchNormalization())(cnn2)\n",
        "Pool2 = keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=2, strides=2))(Norm2)\n",
        "Flat = keras.layers.TimeDistributed(keras.layers.Flatten())(Pool2)\n",
        "lstm1 = keras.layers.LSTM(50,activation ='tanh',return_sequences =True)(Flat)\n",
        "lstm2 = keras.layers.LSTM(1,activation='sigmoid')(lstm1)\n",
        "model = keras.Model(inputs = [input_],outputs=[lstm2])\n",
        "model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_,y_train,epochs=20)\n",
        "model.summary()\n",
        "y_pred=model.predict(X_test_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_Qz0KmCowVa",
        "outputId": "9babf6ca-85e8-45a5-82c7-238abff19992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2979/2979 [==============================] - 61s 19ms/step - loss: 0.4312 - accuracy: 0.8707\n",
            "Epoch 2/20\n",
            "2979/2979 [==============================] - 55s 18ms/step - loss: 0.4127 - accuracy: 0.8898\n",
            "Epoch 3/20\n",
            "2979/2979 [==============================] - 54s 18ms/step - loss: 0.4093 - accuracy: 0.8932\n",
            "Epoch 4/20\n",
            "2979/2979 [==============================] - 55s 18ms/step - loss: 0.4078 - accuracy: 0.8949\n",
            "Epoch 5/20\n",
            "2979/2979 [==============================] - 56s 19ms/step - loss: 0.4073 - accuracy: 0.8951\n",
            "Epoch 6/20\n",
            "2979/2979 [==============================] - 59s 20ms/step - loss: 0.4069 - accuracy: 0.8953\n",
            "Epoch 7/20\n",
            "2979/2979 [==============================] - 56s 19ms/step - loss: 0.4061 - accuracy: 0.8965\n",
            "Epoch 8/20\n",
            "2979/2979 [==============================] - 56s 19ms/step - loss: 0.4057 - accuracy: 0.8966\n",
            "Epoch 9/20\n",
            "2979/2979 [==============================] - 56s 19ms/step - loss: 0.4053 - accuracy: 0.8969\n",
            "Epoch 10/20\n",
            "2979/2979 [==============================] - 57s 19ms/step - loss: 0.4064 - accuracy: 0.8955\n",
            "Epoch 11/20\n",
            "2979/2979 [==============================] - 55s 19ms/step - loss: 0.4053 - accuracy: 0.8968\n",
            "Epoch 12/20\n",
            "2979/2979 [==============================] - 57s 19ms/step - loss: 0.4044 - accuracy: 0.8978\n",
            "Epoch 13/20\n",
            "2979/2979 [==============================] - 55s 18ms/step - loss: 0.4039 - accuracy: 0.8989\n",
            "Epoch 14/20\n",
            "2979/2979 [==============================] - 53s 18ms/step - loss: 0.4036 - accuracy: 0.8987\n",
            "Epoch 15/20\n",
            "2979/2979 [==============================] - 53s 18ms/step - loss: 0.4032 - accuracy: 0.8994\n",
            "Epoch 16/20\n",
            "2979/2979 [==============================] - 53s 18ms/step - loss: 0.4029 - accuracy: 0.8994\n",
            "Epoch 17/20\n",
            "2979/2979 [==============================] - 53s 18ms/step - loss: 0.4022 - accuracy: 0.9010\n",
            "Epoch 18/20\n",
            "2979/2979 [==============================] - 55s 18ms/step - loss: 0.4023 - accuracy: 0.9001\n",
            "Epoch 19/20\n",
            "2979/2979 [==============================] - 55s 18ms/step - loss: 0.4020 - accuracy: 0.9001\n",
            "Epoch 20/20\n",
            "2979/2979 [==============================] - 54s 18ms/step - loss: 0.4021 - accuracy: 0.8996\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, 39, 1)]     0         \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 37, 128)    512       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, None, 37, 128)    512       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, None, 18, 128)    0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, None, 16, 64)     24640     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDis  (None, None, 16, 64)     256       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDis  (None, None, 8, 64)      0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  (None, None, 512)        0         \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 50)          112600    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 1)                 208       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,728\n",
            "Trainable params: 138,344\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "745/745 [==============================] - 5s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qMZAsEhQQPQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
        "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
        "print(\"F1-score: {:.2f}%\".format(f1 * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "WIqWjGTVo1D4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9c49b7fb-2b64-416e-9306-40a3131df6a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-52e98567ae53>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n33539iw8Osr"
      },
      "outputs": [],
      "source": [
        "# X_ = tf.constant(X).reshape(-1,1)\n",
        "\n",
        "# # Create labels (using tensors)\n",
        "# y_ = tf.constant(y).reshape(-1,1)\n",
        "print(X)\n",
        "model.fit(tf.expand_dims(X,axis=-1),y,epochs=100)\n",
        "# model.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPeR598O4Ubs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}